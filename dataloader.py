import torch
import torch.nn
from torch.utils.data import Dataset, DataLoader

import random
import pandas as pd

from tokenization import TOKENIZER, BERT_MODEL, create_out_tensor
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence

#create a directory where the key is a csv. each row has first column as the raw text sentence, and the second col being the 
# path to the file that stores all its lambda terms

DATA_PATH = "/home/mishaalk/projects/def-gpenn/mishaalk/lambdaBERT/data/"

BOS_TOKEN = [[[-3.6689e+00,  2.4402e+00, -2.1537e-02,  1.2105e+00, -2.4603e+00,
           3.3076e-01,  3.1999e+00, -1.2389e+00, -3.0920e-01, -3.9507e+00,
          -1.7301e+00,  6.8464e-01,  5.4974e-01, -2.4822e-01, -5.9577e-01,
           5.2052e+00,  3.8606e+00, -2.1349e+00,  1.1541e+00, -4.1626e+00,
          -1.1677e+00, -1.5630e+00,  6.2513e-01,  2.0005e+00,  9.8942e-01,
           6.5186e-01,  1.0820e+00,  3.4529e+00, -9.2420e-01,  1.7231e+00,
           1.5321e+00,  1.1621e+00, -8.1542e-01,  3.2335e-01,  2.8651e-03,
           8.2287e-02, -2.3766e+00, -4.7895e+00,  1.4318e+00,  2.0294e+00,
          -4.5732e-01, -1.2731e+00, -2.1156e-01,  6.5752e-01,  9.3648e-01,
          -1.4992e+00, -9.8438e-01,  2.0390e+00, -2.1342e-01, -2.7007e-02,
          -2.4574e+00,  2.7368e+00,  3.5540e+00, -5.8367e-01, -4.5383e-01,
           3.4817e-01, -7.1243e-01, -2.9446e+00,  2.5272e+00, -1.7305e+00,
           1.9348e+00, -3.3078e-01,  1.5137e+00,  1.1281e+00,  1.4531e+00,
           5.3586e-01, -2.1398e+00, -2.0416e+00, -3.0380e+00,  2.4514e-01,
          -2.0896e-01,  3.3902e+00, -1.5808e-01,  2.7758e-01, -3.0404e-01,
           1.0977e+00,  2.0630e+00, -1.8649e+00, -5.6617e-01,  1.3122e+00,
          -2.6763e-01,  3.3273e+00, -8.2158e-01,  1.9989e+00,  4.5547e-01,
           1.1566e+00,  1.9333e+00, -1.9291e+00, -2.1718e+00,  1.8472e+00,
           3.6057e+00,  1.1169e+00,  1.3994e+00,  5.6986e-01,  1.0420e+00,
           3.1663e+00, -2.3972e+00, -1.6125e+00, -1.0045e+00,  1.8217e+00,
           1.5062e+00, -1.4974e+00,  1.2083e+00,  1.2379e+00, -2.6465e+00,
          -1.8320e-01, -1.1677e-02, -1.8031e+00, -3.5367e+00,  3.4459e-01,
          -1.8776e+00, -8.2068e-01, -4.5326e+00,  8.3457e-01, -6.5122e-01,
           6.4373e-01,  1.7477e-01,  8.1284e-01,  1.4414e+00, -3.5979e+00,
           1.7330e+00,  4.6250e+00,  5.2094e-01,  2.3520e+00, -1.1263e+00,
          -1.2915e+00, -1.0987e+00,  2.6828e+00, -2.8151e+00, -1.7538e+00,
           2.6284e-01,  2.5745e+00,  7.1778e-01, -2.0363e+00, -2.2285e+00,
          -9.8439e-01,  1.7877e+00, -9.0444e-01,  1.7204e+00, -1.7745e+00,
          -3.8349e+00, -1.3003e+00,  1.3950e+00, -5.3059e+00, -1.3407e+00,
          -1.9233e-01,  1.3572e+00,  1.2490e+00, -1.1795e+00, -2.7445e-01,
           2.8878e+00,  7.2251e-01, -3.2647e+00, -1.8533e-01,  1.4411e+00,
          -1.1472e-01,  3.3833e-01, -7.9783e-02,  1.1917e+00, -7.3812e-01,
          -1.0734e+00, -1.8383e+00, -9.3352e-01, -2.3170e-01,  1.0578e+00,
           2.1976e+00, -1.9722e+00,  3.7592e+00,  1.1878e+00,  3.5546e-01,
          -1.3671e+00, -1.5125e+00, -1.4876e+00,  9.0947e-01,  4.5661e+00,
           4.4485e-01, -1.8777e+00, -1.6309e+00,  2.1995e+00, -5.0757e-01,
          -6.5422e-01,  3.2051e-01, -1.0906e+00,  3.9897e-01,  1.0087e+00,
          -2.7811e+00,  2.0752e+00, -2.0133e+00, -3.5301e+00,  1.2569e+00,
           9.4079e-01, -1.3250e+00,  2.4770e+00,  2.3364e+00,  1.9080e+00,
          -1.2686e-01,  3.6260e-01,  3.7756e-01, -1.0622e+00,  2.3459e-01,
           3.3433e+00,  1.3125e+00, -8.0804e-01,  7.8092e-01, -1.7554e+00,
           4.1115e+00,  3.5641e+00, -1.9766e+00, -4.2137e-01, -2.0583e+00,
           8.8571e-01,  5.5102e-01,  1.5848e+00, -1.7138e+00,  4.1969e+00,
          -4.5579e-01, -1.4262e+00, -1.1373e+00, -1.0317e+00,  4.6225e-01,
           5.6882e-01, -5.6175e-01,  6.4489e-01,  2.8280e+00, -5.8008e-01,
          -9.3355e-01, -1.6808e+00,  2.3813e+00, -4.1217e+00, -1.7416e+00,
          -7.4759e-01, -1.1328e+00,  3.3809e+00, -9.3519e-01, -2.1299e+00,
           5.3426e-01, -3.2739e+00,  7.4194e-01, -5.6117e-02, -1.1607e+00,
          -6.4674e-01, -6.2397e-01, -2.2204e+00,  6.5136e-01,  2.2766e+00,
          -3.5079e+00,  1.0976e+00,  1.9022e+00, -2.7165e-01,  9.8171e-01,
          -4.1218e+00,  1.9596e+00,  2.0161e+00, -2.0022e+00, -2.1963e+00,
           8.3727e-02, -3.0146e-01,  1.1066e+00, -8.3914e-01,  2.4459e+00,
          -2.3986e+00,  1.0196e-01,  5.1124e-02,  1.6712e-01, -3.1186e+00,
          -1.1865e+00, -1.9461e-02, -5.2453e-01,  7.5716e-01,  1.5678e+00,
           1.6795e+00,  2.7083e+00, -1.0386e+00,  1.2379e+00,  4.1561e-01,
          -1.5569e-02,  1.9209e+00,  3.8295e-01, -8.6523e-01,  8.7471e-01,
          -2.9893e+00,  1.0092e-01,  2.4576e+00, -2.3532e+00,  8.7681e-01,
           2.7459e+00, -4.7752e-01, -1.4212e+00, -1.5257e+00, -7.6688e-01,
          -2.4888e+00,  2.7338e+00,  8.6566e-01, -6.9593e-01, -1.9500e+00,
           1.5324e-01, -4.2616e+00,  2.4861e+00,  3.5253e-01, -3.4715e-01,
          -1.4380e+00,  3.7470e+00,  7.3671e-01,  1.6693e+00, -5.8753e-01,
           1.2168e+00, -2.2770e+00,  4.7523e-01, -3.7833e+01, -1.0326e-02,
           9.5536e-01,  3.8326e-01,  2.1803e-01,  2.7902e+00, -2.5307e-01,
          -9.4137e-01, -1.2095e+00,  1.3520e+00,  1.2821e+00, -9.7364e-01,
          -5.7178e-02,  2.0231e+00, -1.9617e-01, -3.9054e+00, -2.6907e+00,
           1.2908e+00, -7.6829e-01,  8.4546e-01, -4.8311e-01, -3.1391e+00,
          -7.9371e-01, -3.4470e+00, -1.1758e+00, -1.0922e+00, -1.5641e-01,
          -3.0129e-01, -1.7964e+00, -1.4934e+00, -7.5530e-01, -1.7397e+00,
           4.9764e-01,  1.9839e-01,  2.3723e+00,  6.1550e-01,  5.6692e-01,
          -1.0164e+00,  8.7963e-01,  7.3754e-01, -4.2780e+00, -5.8857e-02,
          -1.7344e+00,  4.4256e+00, -4.5191e-01, -1.7486e+00,  5.8358e+00,
           3.2088e-01,  4.9214e-02, -2.4928e-01,  7.9509e-01,  2.2763e+00,
           3.1138e+00, -2.9966e+00,  5.7807e-01, -7.3959e-01, -2.8945e+00,
           4.6503e-01,  1.7215e+00, -2.2621e+00,  1.4991e+00, -9.9159e-01,
          -4.4846e-01,  1.5643e+00, -3.0853e-01,  8.8856e-01, -3.4468e+00,
          -1.9197e+00,  1.1705e+00,  4.6882e-01, -8.4509e-02,  1.4303e+00,
           6.5290e-01, -1.0487e+01,  5.1943e-01, -3.1655e+00,  1.4047e+00,
          -9.9193e-01,  3.0779e+00, -1.9534e-01, -8.1457e-01,  1.0863e+00,
          -1.7182e+00,  2.3143e+00,  9.4046e-01,  2.3844e+00, -2.5633e-01,
           1.5181e-02,  8.7758e-01,  2.5952e-01,  1.9243e+00,  1.5420e+00,
          -6.2037e-01,  3.3515e+00, -1.5982e+00,  1.6571e+00,  2.7708e+00,
           1.1844e+00,  1.1716e-01, -3.3734e+00,  3.0711e-01, -1.4233e+00,
          -2.7512e-01,  1.9695e+00,  5.9914e-01, -9.9266e-01, -6.8715e-01,
           8.7399e-01,  5.3627e-01, -8.8823e-01,  3.0038e+00, -1.9242e+00,
           3.3841e+00, -4.7807e-01, -5.7001e-01, -3.6979e+00, -2.4346e+00,
           1.7501e+00,  2.6932e+00,  9.0837e-01,  1.0527e+00, -1.6508e-01,
          -6.7452e-01, -2.6512e+00, -2.2807e+00, -1.6110e+00, -2.5342e+00,
           2.4484e-01,  3.0416e-01, -2.0309e+00,  1.1341e+00, -1.2117e-01,
           4.8610e-01, -7.5661e-01,  1.9587e+00,  1.1363e+00,  1.1284e-01,
           1.1326e+00, -1.5264e+00,  2.5952e+00,  4.7086e+00, -5.6708e-01,
          -3.6260e+00, -1.8531e+00,  1.3395e+00,  1.0768e+00, -1.9033e+00,
          -7.8304e-01, -1.3467e+00,  2.6448e+00,  1.8393e+00, -7.7901e-01,
           4.4437e-02, -2.7256e-01,  3.7711e+00,  5.8149e-01,  9.9020e-01,
          -5.1605e-01,  1.4362e+00, -3.6096e+00,  1.5592e+00, -7.4219e-01,
           8.8748e-02,  5.0672e-01, -1.3457e+00, -4.1003e-01, -8.6936e-01,
           1.4318e-01, -2.9117e+00, -9.0610e-01, -3.1768e+00, -1.3720e+00,
          -2.0759e+00,  2.9496e-02,  1.5689e+00,  2.5413e+00, -2.3992e+00,
          -3.3651e-01, -1.1586e+00, -9.7366e-01,  6.0934e-02,  2.0487e+00,
          -1.7404e+00, -1.5162e-01,  1.7170e+00, -1.2509e+00, -2.8174e-01,
           7.6295e-02, -1.0310e+00,  1.1917e+00,  3.0018e+00, -1.0670e+00,
          -2.6798e+00,  3.1534e-01, -1.2595e+00, -7.4522e-01, -1.5680e+00,
          -3.6976e+00,  8.7639e-02,  1.3121e+00, -2.0376e+00,  1.9128e+00,
          -1.4548e+00, -6.4030e-01, -2.3170e-01,  6.0072e-01,  1.1608e+00,
           5.7274e-01, -3.6477e+00, -1.8463e+00, -7.5535e-01,  1.4198e+00,
           1.1242e+00, -1.3156e+00,  1.1793e+00,  1.2967e+00, -2.3230e+00,
           3.5238e+00,  7.1538e-01,  1.6560e+00, -1.5568e+00, -1.1180e-01,
           6.9523e-01,  1.3284e+00, -1.5114e+00, -1.6369e+00,  8.9926e-01,
          -3.4816e-01, -6.1807e-01, -3.1471e-01, -1.4112e+00,  4.7090e-01,
          -1.6523e+00, -1.0716e+00, -9.8341e-01, -1.5954e+00,  8.2798e-01,
           2.7397e+00,  2.7901e+00,  2.0564e+00,  2.0415e+00,  1.9032e+00,
          -1.0612e+00,  3.0899e+00, -9.5817e-01, -1.3198e+00,  3.2592e+00,
           1.8579e+00,  2.9191e-01,  4.5997e+00,  1.5530e+00, -1.3887e+00,
           2.7900e+00, -3.0862e+00,  2.7733e-01, -2.2784e+00, -2.4700e+00,
          -2.5223e+00,  5.8715e-01, -1.9622e+00,  1.1530e+00,  9.1822e-01,
           8.0351e-01, -1.8940e+00,  1.0300e+00, -2.4120e+00,  1.2834e+00,
          -1.6318e+00,  5.7325e+00, -3.8909e-01,  3.5115e+00, -2.5856e+00,
          -3.8418e+00,  9.1004e-01,  2.8152e+00,  1.8885e+00, -2.3041e+00,
          -1.4502e+00, -6.5796e-01,  2.6010e-01, -3.5697e+00, -2.2041e+00,
           5.4604e-01,  3.8948e-01, -3.5117e+00, -4.6602e+00, -1.6670e-01,
           5.3252e-01, -1.0770e+00, -7.7978e-01, -1.7140e+00,  9.4847e-01,
          -1.6561e+00,  2.8742e-01, -3.3726e+00, -1.3101e+00, -8.6766e-01,
           6.6056e-01, -2.2209e+00,  3.3412e+00, -1.8386e-01,  1.2432e+00,
           4.1607e-01, -1.4121e+00,  9.0122e-01,  6.3345e-01,  4.7945e-01,
          -1.4343e-01,  3.3837e+00,  1.5840e-01,  6.4397e-01,  2.0294e-01,
           8.0033e-01,  1.9855e+00,  1.1257e+00, -4.4760e-01,  9.9550e-01,
           1.5427e+00,  1.5657e+00,  6.9124e-01,  2.7764e+00, -1.2489e+00,
           1.1461e+00, -1.5171e+00, -1.0097e+00, -2.1816e+00, -5.6329e-01,
          -3.3383e+00,  5.7826e-02,  3.3460e-01, -2.3513e+00,  3.9646e+00,
          -2.8275e+00,  5.5523e-02, -2.4295e-01,  9.4484e-01,  1.2677e-01,
          -7.2816e-01, -2.3269e+00,  1.1600e+00,  1.5394e-01,  1.7852e+00,
           3.2923e+00, -1.0077e+00, -3.3549e-02,  3.6380e+00, -2.5722e+00,
          -8.2362e-01, -4.5247e+00, -1.2393e+00,  4.0090e+00, -3.2927e+00,
           9.9201e-01,  2.7006e-01,  7.4364e-01, -5.8848e-01, -6.9845e-01,
          -4.4076e+00, -2.6909e+00,  2.0762e+00,  3.8291e-01,  4.2993e+00,
          -1.4442e-02, -1.5222e+00, -1.0729e+00, -1.6858e+00, -9.8093e-01,
           3.0854e+00, -1.2389e+00,  1.0579e+00, -4.6502e-01, -1.1923e+00,
          -1.6956e+00,  4.3489e+00,  3.4216e+00, -6.6558e-01,  2.4954e+00,
           2.1687e-01, -1.9190e+00,  7.2786e-01,  1.6080e+00, -4.3063e+00,
           1.2269e+00,  3.8355e+00,  2.0450e+00,  1.3208e+00, -1.4179e+00,
          -3.4192e+00,  2.0998e-01, -2.1146e+00,  3.2434e-01, -3.0002e-01,
          -3.2157e-01,  7.2503e-01, -3.8766e+00,  1.8853e+00,  3.2546e-01,
           1.5398e+00,  2.0569e+00, -4.2092e+00, -7.4235e-01,  3.8446e+00,
          -8.6614e-01, -2.6590e-01, -1.7130e+00, -6.4784e-01, -1.5010e+00,
           3.7457e+00, -2.6665e-01, -2.2461e+00, -1.7666e-01,  1.1233e+00,
          -1.2118e+00,  1.2084e+00, -2.1611e+00, -3.6623e+00,  1.0840e+00,
           1.2580e+00,  1.3050e+00,  6.8156e-01,  2.7339e+00,  1.9533e+00,
          -1.6284e-01, -1.9211e+00, -3.8614e+00,  1.0336e+00, -2.7587e+00,
           3.8808e+00,  1.5620e+00,  3.8866e+00, -2.0343e+00, -8.3985e-01,
           6.7332e-01, -8.8383e-01,  6.4402e-01,  4.1193e-01, -1.0125e+00,
          -3.5162e+00, -7.0233e-01, -3.5665e+00,  7.8189e-02,  5.7313e-01,
          -7.5592e-02, -2.0951e+00, -2.7254e+00, -1.3589e+00, -2.3049e+00,
           3.5899e-01, -3.5669e-01, -3.5363e-01,  1.5588e+00, -2.3448e+00,
          -1.8737e+00,  1.2763e+00,  1.1504e+00, -1.4568e+00,  1.8637e-02,
          -3.4842e-01, -7.8625e-01,  3.0266e+00]]]

SEP_TOKEN = [[[ 7.6267e-01,  8.9944e-03, -3.6583e-01,  1.9516e-01, -4.8590e-01,
          -2.4512e-01,  5.0226e-01, -4.1119e-01,  4.1043e-01, -9.3080e-02,
           2.3451e-01,  6.1152e-02,  5.1634e-01, -1.2909e-01, -5.4241e-01,
          -6.7891e-01,  1.6271e-01, -3.3280e-01,  2.9146e-01, -1.7934e-03,
          -1.8548e-01,  2.7439e-01,  3.2378e-01, -3.9362e-01,  3.9103e-02,
          -4.8627e-01, -4.8901e-01,  1.3215e-01, -1.6837e-01, -8.3485e-01,
          -1.1902e-01, -3.7513e-01, -4.1434e-01,  6.5679e-01,  1.6730e-01,
           1.0167e-01,  2.7230e-01, -1.8922e-01, -5.6889e-01, -1.6211e-01,
          -3.0427e-01,  2.1588e-01, -7.1898e-02,  2.5038e-01,  5.4332e-02,
          -2.7465e-02,  6.9104e-01,  6.2793e-01,  4.2105e-02,  1.0751e+00,
           4.9342e-01,  3.7197e-01, -1.7328e-01,  4.6306e-01,  8.5680e-02,
           3.0679e-01,  2.1996e-01, -1.3788e-01,  1.8176e-01,  7.9807e-01,
           1.4034e-01,  2.8091e-01, -1.3855e-01, -4.7647e-01,  6.6795e-01,
          -6.7147e-03, -4.6838e-02, -3.2756e-01, -3.2389e-01, -1.9392e-01,
          -5.9283e-01, -8.2135e-01,  5.6446e-01,  3.4074e-01,  7.0208e-02,
           7.2009e-01, -3.9942e-01,  5.4239e-01, -4.1528e-01, -1.1378e-01,
           2.4622e-01,  6.1872e-03,  1.6288e-01,  1.3336e-01, -8.3746e-02,
          -9.4092e-02, -8.6415e-01, -1.0402e-01, -6.7240e-01, -4.0488e-01,
           3.5545e-01,  5.2807e-01, -1.5584e-01,  3.0006e-01, -1.5826e-01,
           2.2644e-01, -2.9865e-01,  1.5775e-01,  4.6667e-02,  3.2529e-01,
           4.1341e-01, -1.6814e-01,  5.7357e-02,  8.8873e-01, -1.2910e-01,
          -1.9431e-01,  6.2281e-01,  2.2408e-01,  3.8858e-01,  9.1565e-01,
           9.2464e-01, -2.9152e-02,  6.7837e-01,  2.1678e-01, -1.3357e-01,
          -2.5276e-01,  1.1788e-01, -3.1780e-02,  2.9165e-01, -2.0494e-01,
          -4.0776e-01, -4.2348e-01,  3.4800e-01,  2.8637e+00, -3.2686e-01,
          -5.0334e-02,  2.6012e-01, -6.1795e-01,  2.2880e-01, -2.4557e-01,
          -1.9044e-01,  1.3346e-01,  2.7117e-01,  4.7136e-01, -3.4187e-01,
           7.4806e-01, -1.8126e-01,  1.0366e-01, -7.0490e-01,  1.2994e-01,
          -9.5094e-03,  2.4216e-01,  3.6581e-01, -1.0569e+00,  3.2195e-04,
           5.5790e-01,  8.3230e-01,  3.4033e-01,  1.0215e+00, -6.3070e-01,
           9.3840e-01, -5.0309e-01, -3.6165e-02, -1.4434e-01, -5.6641e-01,
          -2.2899e-01, -1.8544e-01, -1.8859e-03,  4.4514e-01,  7.4233e-01,
           2.9224e-01,  3.2790e-01,  6.6591e-01,  4.5329e-01, -5.1144e-01,
           5.5084e-01, -8.6476e-01, -1.2654e-01,  3.0004e-01,  6.8516e-01,
          -3.7960e-01, -2.3610e-03, -1.6699e-02, -2.1314e-01, -2.8445e-01,
          -3.4513e-01, -3.4324e-01, -2.4967e-01,  1.3239e-01, -9.4867e-01,
          -1.5129e+01, -6.0427e-01,  5.6567e-01,  1.6651e-02, -6.0294e-02,
          -3.8575e-01, -8.4208e-01, -5.4401e-01, -6.5214e-01, -8.4426e-01,
           2.6389e-01,  6.4038e-02, -7.1790e-01,  9.3770e-01,  3.5451e-01,
          -5.4737e-01, -1.0393e-03,  1.3554e-01, -6.0948e-01, -1.3363e-01,
          -2.4515e-01, -2.8462e-01, -2.6447e-01,  4.8226e-01,  1.5910e-01,
          -2.0365e+00, -2.0934e-02, -1.5231e-01,  1.8443e-01,  8.7268e-02,
          -9.9519e-01,  5.8023e-01,  1.8951e-02, -9.1640e-01, -1.5140e-01,
          -1.5935e-01,  1.5242e-01, -2.6932e-01, -9.5103e-01,  3.1548e-01,
          -5.1498e-01, -9.0318e-01, -3.9464e-01, -1.9773e-01,  5.7779e-01,
          -2.0494e+00, -2.1752e-02,  7.1961e-01,  6.5953e-01,  3.9062e-01,
           3.3534e-01, -1.8978e-01,  7.3613e-01,  1.8310e-01, -2.8214e-01,
          -1.7986e-01,  3.2193e-01, -1.0608e-01, -5.2542e-01, -1.9458e-01,
          -4.0921e-01,  2.2369e-01,  2.2311e-01, -5.8487e-01, -1.0249e-01,
          -6.1244e-01, -1.4699e-01,  3.9656e-01,  7.0173e-01, -2.6571e-01,
           1.2863e-01, -9.9597e-01,  4.1185e-01, -5.0390e-01,  1.8710e-01,
           6.3707e-01,  1.2322e-02,  1.0033e-01,  7.1355e-01, -6.8427e-02,
           2.0072e-01,  3.9953e-01,  4.6771e-01, -1.6556e-02, -6.8507e-01,
           5.6382e-01,  1.7322e-01, -1.8064e-02, -3.7863e-01,  3.0163e-01,
           3.7296e-01, -1.0239e-01, -2.2087e-02,  6.1339e-01,  1.5526e-01,
          -9.6819e-01,  1.1019e-01, -1.1808e-01,  3.1445e-01, -3.2270e-01,
          -7.6286e-01,  3.5425e-01, -1.9528e-01,  5.1242e-01,  4.6548e-01,
          -6.1083e-01, -1.0123e+00, -3.2977e-01,  1.4745e-01, -1.0458e+00,
          -4.2121e-01, -4.2424e-01,  4.5558e-01,  5.8747e-01, -3.1613e-01,
           4.2754e-01,  6.3993e-01, -2.2840e-01, -4.5763e-01, -2.7863e-01,
          -1.3649e-01, -1.0505e+00,  1.5676e-01, -5.7440e-01, -7.4964e-01,
           1.9569e-01,  4.1389e-01, -9.4425e-02,  5.2835e+00,  1.7685e-01,
           2.9219e-01, -8.5532e-01,  3.5894e-01,  3.1275e-01, -1.4534e-01,
          -4.5474e-01, -1.6707e-01, -4.2704e-01, -2.6621e-01, -6.7241e-01,
           7.3905e-01,  1.7540e-01, -2.9537e-01,  8.9223e-01,  9.9420e-02,
          -1.2276e-01, -9.4053e-01, -5.5920e-01, -5.5844e-02,  5.5859e-01,
          -1.0266e-01, -4.8815e-01,  8.1300e-01, -4.6080e-01, -3.5023e-01,
          -6.5827e-02,  3.6993e-01,  3.3747e-01, -9.5957e-02, -7.9319e-01,
          -1.0044e-02, -7.2068e-01, -1.0642e-02,  8.9776e-02, -4.1134e-01,
           3.5837e-01,  5.8001e-01,  1.3355e-01, -2.6359e-01,  7.8351e-01,
           3.2953e-02, -5.7745e-01, -6.7663e-01,  3.0995e-02, -6.1204e-01,
          -5.9076e-01, -5.0614e-01,  5.7643e-01,  1.0008e+00, -1.3270e-01,
          -5.9896e-01, -7.5989e-02, -5.4924e-01, -8.3733e-01,  3.1266e-01,
           1.3918e-01, -3.7781e-03, -4.8147e-01,  5.2728e-01, -4.1273e-01,
           9.5344e-02,  5.0207e-01, -4.1334e-01,  1.2792e-01,  4.6800e-01,
           2.9954e-02, -2.8660e-01,  2.6861e-01,  1.7268e-01, -6.4009e-01,
          -7.0854e-01, -9.2937e+00, -4.1905e-01,  2.3547e-01, -5.7396e-01,
           1.5430e-01,  7.9481e-01, -1.5178e-01,  6.0122e-01,  2.7130e-01,
          -3.6474e-01, -1.9536e-01, -8.6911e-02, -2.5349e-01,  1.5560e-03,
          -1.0912e-01,  2.4510e-01, -4.9589e-01,  3.2070e-01,  5.9336e-01,
           6.0548e-01,  1.4850e-01, -2.8809e-01,  5.2043e-01,  3.5414e-01,
           3.4432e-01, -2.1292e-01,  2.3703e-01, -9.9204e-01, -5.0474e-02,
           5.8424e-01, -6.9862e-02,  8.1304e-02,  5.4982e-01, -7.7700e-01,
          -3.9368e-01, -8.8066e-01,  1.3203e-01,  7.4052e-02, -7.7313e-01,
           4.9348e-01,  3.9261e-02, -1.5013e-01, -4.1452e-01,  2.6893e-01,
           8.0561e-01, -1.6230e-01,  3.0457e-01,  8.5209e-01, -4.5283e-01,
           4.4758e-01,  2.1153e-01,  1.9122e-01, -2.9400e-01, -1.8149e-03,
          -5.6515e-02,  3.3852e-01, -3.4784e-01,  1.8784e-02,  1.3956e-01,
          -3.5183e-01, -2.5924e-01, -5.0715e-01,  4.6042e-01,  1.0259e+00,
           7.2754e-01,  7.8185e-01,  5.2936e-01,  3.4169e-01,  7.6544e-01,
          -8.1090e-01, -4.9539e-02, -4.1586e-01,  7.4894e-01, -6.3757e-01,
           2.5667e-01,  5.9575e-01,  1.3454e-01,  2.6268e-01,  5.0265e-01,
           6.9063e-03, -8.4378e-01,  3.0719e-01, -5.6659e-01, -5.9044e-02,
          -1.4162e-01, -7.0815e-01,  1.5517e-01, -8.1626e-01,  4.3382e-01,
           1.3387e-01,  7.4050e-02, -7.3634e-01, -8.6683e-01,  3.6545e-01,
          -1.9162e-01, -3.5771e-01,  1.8371e-01,  1.8611e-01, -2.9048e-01,
           7.4161e-01, -6.1946e-01, -4.3144e-02,  4.5234e-01, -4.7187e-01,
           6.1956e-02, -3.8249e-01, -2.7119e-01,  1.1821e-01, -7.4638e-01,
          -9.1147e-02, -6.1284e-02,  3.7037e-02,  4.5445e-01, -2.6089e-01,
          -1.4144e-01,  7.7364e-01, -2.6355e-01,  6.2408e-01, -3.1994e-01,
          -5.4861e-01,  1.6871e-01, -6.5583e-01,  2.9526e-01,  1.6848e-01,
          -3.8409e-01,  3.5326e-01,  3.1079e-01,  8.1789e-01,  2.7098e-01,
          -7.3080e-01,  2.1496e-02, -1.5928e-01,  1.7852e-01,  1.5640e-01,
           1.0452e-01, -1.0070e-01, -1.1427e-01,  3.8779e-01, -3.5948e-01,
          -1.8059e-01, -3.9174e-01,  5.6886e-01,  4.9265e-01,  1.5954e-01,
           8.9203e-02,  1.7006e+00,  5.3097e-02, -1.0341e+00, -2.8136e-01,
           2.4944e-01,  6.8839e-01, -9.6698e-02, -1.1488e+00, -5.4622e-01,
           3.4146e-01, -5.2281e-01,  2.8346e-01,  2.1529e-01,  1.3943e+00,
          -6.7269e-01,  9.9402e-02,  1.0060e+00,  6.4180e-01, -1.0933e-01,
           7.2312e-01,  1.2422e-01,  2.9238e-01, -5.7830e-01, -9.3930e-02,
          -1.4249e-01,  2.7422e-01,  1.3893e-01, -6.5254e-02,  4.7121e-01,
           1.2755e+00, -1.4254e-01, -1.2261e+00, -2.8538e-01,  1.1652e-01,
           5.3754e-01, -2.2650e-01, -8.4797e-01, -4.1161e-01, -5.0532e-01,
           1.3985e-01,  7.4497e-02, -4.4024e-01,  6.7407e-02, -4.1831e-02,
           1.0604e+00, -8.6838e-01,  6.4409e-02,  3.0175e-01,  3.4785e-01,
           9.5812e-02,  1.1777e-01,  2.7412e-01,  1.4770e-01,  5.3034e-01,
           9.0887e-02, -2.0598e-01, -1.0444e+00,  5.5126e-01,  5.5259e-01,
          -3.5752e-01, -1.1402e+00, -7.6057e-02,  2.2249e-01,  3.9004e-01,
          -2.6181e-01, -3.7811e-01, -3.3159e-01, -8.0901e-02, -2.6154e-01,
           3.5109e-01, -3.0051e-01, -4.0502e-01,  3.7459e-01, -1.1865e-01,
          -4.4610e-01,  1.6786e-01, -2.5004e-01, -1.5684e-01, -2.9405e-01,
           1.1027e-01, -9.6861e-02,  1.2864e-01, -2.8683e-02,  8.7043e-01,
          -3.2783e-01,  1.3470e-01,  3.6207e-02, -1.8379e-01,  1.6099e-01,
           1.9188e-01,  5.8841e-01,  1.9539e-01, -5.9928e-01,  4.2484e-01,
           1.3820e-01,  6.5911e-01, -1.0275e+00, -2.2975e-01, -1.2084e-01,
          -4.8366e-01,  1.5273e-01,  4.4026e-01,  2.5293e-03, -6.4945e-02,
          -1.0577e-01, -1.1782e-01,  7.1818e-02,  2.5648e-01, -9.5896e-01,
          -3.1243e-01, -8.6928e-01,  8.1597e-02,  3.4098e-01,  6.9092e-01,
          -4.6632e-02,  1.6311e-01,  2.8601e-01, -1.7308e-01, -1.1959e-02,
           1.1438e+00,  1.7281e-01, -8.8783e-01,  3.4216e-01, -7.1350e-02,
          -3.7813e-01,  3.5796e-01, -3.6328e-01,  2.1710e-01,  2.1130e-01,
          -1.4458e-01,  1.5907e-01, -3.9699e-01,  3.5405e-01,  1.0186e-01,
          -8.3606e-02, -5.2517e-02, -8.4341e-01, -6.6619e-03,  3.9718e-01,
          -6.2875e-02,  7.2003e-02, -2.2825e-02, -4.5825e-01, -6.9813e-01,
          -9.2843e-01, -5.1929e-01,  7.3204e-01,  1.2509e+00, -1.7067e-01,
          -7.0838e-01, -9.1830e-01,  8.8002e-02,  4.2317e-01,  5.5825e-01,
          -3.8786e-01, -8.7177e-01, -2.1303e-01,  4.4943e-01, -3.0673e-01,
           3.7002e-01, -7.7388e-01,  7.1990e-02,  7.8776e-01,  7.4956e-02,
          -5.2348e-01, -1.5019e-01,  8.7144e-03, -7.1603e-01, -7.1644e-01,
          -5.4167e-01,  3.8696e-01,  9.1727e-02,  3.6912e-01, -5.2770e-01,
          -3.2090e-01, -4.2872e-01,  5.6781e-01,  2.1204e-01,  2.9984e-01,
           3.6689e-02,  4.2757e-01, -6.6840e-02,  6.1161e-01, -1.2082e-01,
          -1.0982e+00,  3.6305e-01, -5.0004e-01, -1.0481e-01,  1.3113e+00,
          -4.4254e-01,  6.2662e-01, -3.4315e-01, -4.8654e-01,  4.1413e-02,
          -4.9833e+00,  3.8893e-02, -3.4393e-02, -1.7457e-01, -3.9040e-01,
           2.2913e-01,  1.8626e-01,  2.4573e-02,  4.0977e-01, -1.9457e-01,
           4.3656e-01, -5.1829e-02, -5.4883e-01, -4.0841e-01, -3.8384e-01,
           5.8838e-01,  3.6499e-01, -6.1254e-01,  6.7168e-01,  1.6272e-01,
           2.3727e-01, -3.0993e-02,  4.9746e-01, -1.6534e-01, -1.9408e-01,
           8.6395e-02, -3.0598e-01, -5.7015e-01, -3.7614e-02, -1.7181e-01,
           4.5752e-01,  2.1234e-01,  2.3444e-01, -7.2374e-02,  9.5238e-01,
          -3.7161e-01,  6.8616e-01, -4.4355e-01,  1.9432e-01, -2.3774e-01,
          -5.6820e-01, -5.8610e-01, -1.0096e-01, -6.9678e-02,  3.2554e-01,
           1.2490e-01, -7.0842e-01, -4.8841e-01]]]


class LambdaTermsDataset(Dataset):
    def __init__(self, input_sentences_file, main_dir, transform=None, target_transform=None):
        self.main_dir = main_dir
        self.input_sentences = pd.read_csv(input_sentences_file)
        self.transform = transform
        self.target_transform = target_transform
    
    def __len__(self):
        return len(self.input_sentences)

    def __getitem__(self, index):
        sentence = self.input_sentences.iloc[index, 0]
        if sentence[0] == '""': sentence = sentence[1]
        if sentence[-1] == '""': sentence = sentence[:-1]
        path = self.input_sentences.iloc[index, 2]
        path = DATA_PATH + path[len("lambdaBERT/data/"):]
        with open(path, 'r') as f:
            lambda_terms = f.readlines()[0].strip()

        # remove the ")" from the lambda_term:
        lambda_terms = lambda_terms.replace(")", "")

        if self.transform:
            sentence = self.transform(sentence)
        if self.target_transform:
            lambda_terms = self.target_transform(lambda_terms)
        
        return sentence, lambda_terms
    
class ShuffledLambdaTermsDataset(Dataset):
    def __init__(self, input_sentences_file, main_dir, transform=None, target_transform=None):
        self.main_dir = main_dir
        self.input_sentences = pd.read_csv(input_sentences_file)
        self.transform = transform
        self.target_transform = target_transform
    
    def __len__(self):
        return len(self.input_sentences)

    def __getitem__(self, index):
        sentence = self.input_sentences.iloc[index, 1]
        if sentence[0] == '""': sentence = sentence[1]
        if sentence[-1] == '""': sentence = sentence[:-1]
        path = self.input_sentences.iloc[index, 2]
        path = DATA_PATH + path[len("lambdaBERT/data/"):]
        with open(path, 'r') as f:
            lambda_terms = f.readlines()[0].strip()

        # remove the ")" from the lambda_term:
        lambda_terms = lambda_terms.replace(")", "")

        sent_embs, target_embs, target_tokens, lambda_index_mask, var_index_mask_no, app_index_mask = torch.load(path.replace("txt", "pt"))#create_out_tensor(sentence, lambda_terms)

        #attach the CLS and SEP tokens to the start and end of target_embs?

        if len(target_embs) == 0:
            if "section_6186/4.txt" in path or "section_4652/5.txt" in path or "section_7065/2.txt" in path or "/section_5020/4.txt" in path:
                target_embs = torch.Tensor()
                target_tokens = []
                assert len(lambda_index_mask) == 0 
                assert len(var_index_mask_no) == 0
                assert len(app_index_mask) == 0

                # lambda_index_mask.append(0)
                # var_index_mask_no.append(0)
                # app_index_mask.append(0)
            else:
                raise Exception

        
        #sep token for target embedding:
        target_embs = torch.cat([torch.tensor(BOS_TOKEN).squeeze(0), target_embs, torch.tensor(SEP_TOKEN).squeeze(0)], dim=0)
        target_tokens = [101] + target_tokens + [102]
        lambda_index_mask = [0] + lambda_index_mask + [0]
        var_index_mask_no = [0] + var_index_mask_no + [0]
        app_index_mask = [0] + app_index_mask + [0]
        
        return sent_embs, target_embs, target_tokens, lambda_index_mask, var_index_mask_no, app_index_mask

def shuffled_collate(batch):
    sent_embedding, lambda_term_embedding, lambda_term_tokens, lambda_mask, var_mask, app_mask = zip(*batch)
    
    sent_embedding, lambda_term_embedding, lambda_term_tokens, lambda_mask, var_mask, app_mask = [sent.squeeze(0) for sent in sent_embedding], [lambda_term.squeeze(0) for lambda_term in lambda_term_embedding], [torch.tensor(sent, dtype=torch.float32) for sent in lambda_term_tokens],[torch.tensor(sent, dtype=torch.bool).squeeze(0) for sent in lambda_mask], [torch.tensor(var, dtype=torch.bool).squeeze(0) for var in var_mask], [torch.tensor(app, dtype=torch.bool).squeeze(0) for app in app_mask]

    sent_embedding_batched = pad_sequence(sent_embedding, batch_first=True, padding_value = 0)
    try:
        lambda_term_embedding_batched = pad_sequence(lambda_term_embedding, batch_first=True, padding_value = 15)
        lambda_term_tokens_batched = pad_sequence(lambda_term_tokens, batch_first=True, padding_value = 0)
    except:
        print([lambda_term.shape for lambda_term in lambda_term_embedding])
        raise Exception
    var_mask_batched = pad_sequence(var_mask, batch_first=True, padding_value = 0)
    lambda_mask_batched = pad_sequence(lambda_mask, batch_first=True, padding_value = 0)
    app_mask_batched = pad_sequence(app_mask, batch_first=True, padding_value = 0)

    lambda_pad_mask = lambda_term_embedding_batched == 15
    lambda_term_embedding_batched = lambda_term_embedding_batched.masked_fill(lambda_pad_mask, 0)

    #extend the masks
    # lambda_mask_batched = lambda_mask_batched.unsqueeze(-1).expand(-1, -1, lambda_term_embedding_batched.size(-1))
    # var_mask_batched = var_mask_batched.unsqueeze(-1).expand(-1, -1, lambda_term_embedding_batched.size(-1))
    # app_mask_batched = app_mask_batched.unsqueeze(-1).expand(-1, -1, lambda_term_embedding_batched.size(-1))
    #contract the mask
    lambda_pad_mask = lambda_pad_mask.sum(-1) >= 1

    return sent_embedding_batched, lambda_term_embedding_batched, lambda_term_tokens_batched, var_mask_batched, lambda_mask_batched, app_mask_batched, lambda_pad_mask


def data_init(batch_size, mode=0, shuffled=False):
    
    #load in the tokenizer
    # tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
    if not shuffled: dataset = LambdaTermsDataset(DATA_PATH + 'input_sentences.csv', DATA_PATH + 'lambda_terms/')
    else: dataset = ShuffledLambdaTermsDataset(DATA_PATH + 'input_sentences.csv', DATA_PATH + 'lambda_terms/')
    #split the datset 70 20 10 split
    train_size = int(0.7 * len(dataset))
    val_size = int(0.2 * len(dataset))
    test_size = len(dataset) - train_size - val_size
    
    if mode == 2:
        train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])
        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=12, collate_fn=shuffled_collate)
        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=12, collate_fn=shuffled_collate)
        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=12, collate_fn=shuffled_collate)
        return train_dataloader, val_dataloader, test_dataloader
    elif mode == 0:
        train_size += test_size

        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])

        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=9, collate_fn=shuffled_collate)
        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=9, collate_fn=shuffled_collate)

        return train_dataloader, val_dataloader
    else:
        #just one dataloader
        train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=9, collate_fn=shuffled_collate)
        return train_dataloader



            
